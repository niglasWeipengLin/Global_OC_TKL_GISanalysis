

"""
åˆ¤æ–­æ¹–æ³Šåœ¨å“ªä¸ªç”Ÿæ€åŒºå†… - ç¬¬ä¸€æ­¥ï¼šå®Œå…¨åŒ…å«
"""
import geopandas as gpd
import pandas as pd
import os
from tqdm import tqdm

# å‚æ•°è®¾ç½®
hydrolake_path = r'G:\ren\CBR\GLAKES\natural_lakes.gpkg'
ecoregion_path = r'G:\ren\CBR\Ecoregions\ecoregions_dissolved_by_biome.shp'
output_dir = r'G:\ren\æ¹–æ³Šç”Ÿæ€åŒºç»Ÿè®¡\hydrolake_ecoregionbatches'
remain_dir = os.path.join(output_dir, 'lake_remain')
chunk_size = 10000
start_batch = 0  # âœ… ä¿®æ”¹ä¸ºä½ å¸Œæœ›å¼€å§‹è¿è¡Œçš„æ‰¹æ¬¡ç¼–å·

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
os.makedirs(output_dir, exist_ok=True)
os.makedirs(remain_dir, exist_ok=True)

# è¯»å–æ•°æ®
gdf_lakes = gpd.read_file(hydrolake_path)
gdf_regions = gpd.read_file(ecoregion_path)
gdf_regions = gdf_regions.to_crs(gdf_lakes.crs)

# æ£€æŸ¥æ¹–æ³Šå›¾å±‚ä¸­æ˜¯å¦å­˜åœ¨ Area_bound å­—æ®µ
if 'Area_bound' not in gdf_lakes.columns:
    raise ValueError("å­—æ®µ 'Area_bound' ä¸å­˜åœ¨äº natural_lakes.gpkg ä¸­ï¼Œè¯·ç¡®è®¤å…¶æ¥æºæ­£ç¡®ã€‚")

# åˆ†æ‰¹å¤„ç†
chunks = [gdf_lakes.iloc[i:i+chunk_size] for i in range(0, len(gdf_lakes), chunk_size)]
total_batches = len(chunks)

# å­˜å‚¨æœªåŒ¹é…æ¹–æ³Š
remain_list = []

# é¡ºåºå¤„ç†æ¯ä¸€æ‰¹
for idx in tqdm(range(start_batch, total_batches), desc="å¤„ç†è¿›åº¦"):
    chunk = chunks[idx].copy()
    print(f"ğŸ”¹ æ­£åœ¨å¤„ç†ç¬¬ {idx+1}/{total_batches} æ‰¹...")

    # ç©ºé—´è¿æ¥ï¼ˆæ¹–æ³Šåœ¨ç”Ÿæ€åŒºå†…ï¼‰
    joined = gpd.sjoin(chunk, gdf_regions[['BIOME_NAME', 'geometry']],
                       how='left', predicate='within')

    # åŒ¹é…æˆåŠŸ
    matched = joined[~joined['BIOME_NAME'].isna()]
    out_path = os.path.join(output_dir, f"lake_batch_{idx+1:05}.xlsx")
    matched[['Lake_id', 'BIOME_NAME', 'Area_PW', 'Area_bound']].to_excel(out_path, index=False)

    # åŒ¹é…å¤±è´¥
    remain_chunk = chunk[joined['BIOME_NAME'].isna()]
    if not remain_chunk.empty:
        remain_list.append(remain_chunk)

# åˆå¹¶å‰©ä½™æ¹–æ³Šå¹¶è¾“å‡º
if remain_list:
    remain_gdf = pd.concat(remain_list).drop_duplicates(subset='Lake_id')
    remain_gdf = gpd.GeoDataFrame(remain_gdf, geometry='geometry', crs=gdf_lakes.crs)
    remain_path = os.path.join(remain_dir, 'remain.shp')
    remain_gdf.to_file(remain_path)
    print(f"â— æœªè¢«å®Œå…¨åŒ…å«çš„æ¹–æ³Šå·²ä¿å­˜è‡³ï¼š{remain_path}")

print("âœ… æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼")
"""
åˆ¤æ–­æ¹–æ³Šåœ¨å“ªä¸ªç”Ÿæ€åŒºå†… ç¬¬3æ­¥ ä¸å¤šä¸ªç”Ÿæ€åŒºç›¸äº¤æ—¶ è®¡ç®—é¢ç§¯å æ¯” å–æœ€å¤§çš„ä½œä¸ºæ¹–æ³Šç”Ÿæ€åŒº
"""


import geopandas as gpd
import pandas as pd
import os
import math

# ========== è·¯å¾„è®¾ç½® ==========
hydrolake_path = r'G:\ren\æ¹–æ³Šç”Ÿæ€åŒºç»Ÿè®¡\hydrolake_ecoregionbatches\lake_remain\lake_remain2\remain2.shp'
ecoregion_path = r'G:\ren\CBR\Ecoregions\ecoregions_dissolved_by_biome.shp'
output_dir = r'G:\ren\æ¹–æ³Šç”Ÿæ€åŒºç»Ÿè®¡\hydrolake_ecoregionbatches\lake_remain\lakeremain3'

os.makedirs(output_dir, exist_ok=True)

# ========== è¯»å–æ•°æ® ==========
lakes_all = gpd.read_file(hydrolake_path)
ecoregions = gpd.read_file(ecoregion_path)

# ========== æŠ•å½±ç»Ÿä¸€ï¼ˆåŒ—ææŠ•å½±ï¼‰ ==========
target_crs = 'EPSG:3395'
lakes_all = lakes_all.to_crs(target_crs)
ecoregions = ecoregions.to_crs(target_crs)


# ========== åˆ†æ‰¹å¤„ç† ==========
batch_size = 20
num_batches = math.ceil(len(lakes_all) / batch_size)

for i in range(num_batches):
    print(f"ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {i + 1} æ‰¹ / å…± {num_batches} æ‰¹...")

    lakes_batch = lakes_all.iloc[i * batch_size:(i + 1) * batch_size].copy()

    # äº¤é›†è®¡ç®—
    intersections = gpd.overlay(lakes_batch, ecoregions, how='intersection')
    intersections['intersect_area_m2'] = intersections.geometry.area

    # æŒ‰æœ€å¤§äº¤é¢ç§¯é€‰æ‹©ç”Ÿæ€åŒº
    intersections = intersections.sort_values('intersect_area_m2', ascending=False).drop_duplicates('Lake_id')

    # æå–ç”Ÿæ€åŒºå­—æ®µ
    ecoregion_cols = [col for col in intersections.columns if col not in lakes_batch.columns and col not in ['geometry', 'intersect_area_m2']]
    intersections_df = intersections[['Lake_id', 'intersect_area_m2'] + ecoregion_cols]

    # åˆå¹¶å±æ€§
    lakes_batch_df = lakes_batch.drop(columns='geometry')
    matched = pd.merge(lakes_batch_df, intersections_df, on='Lake_id', how='left')

    # æ‹†åˆ†åŒ¹é…å’ŒæœªåŒ¹é…
    matched_valid = matched[matched['intersect_area_m2'].notna()]
    unmatched_ids = matched[matched['intersect_area_m2'].isna()]['Lake_id']
    unmatched_gdf = lakes_batch[lakes_batch['Lake_id'].isin(unmatched_ids)]

    # è¾“å‡º Excel
    excel_out = os.path.join(output_dir, f'batch_{i + 1}_matched.xlsx')
    matched_valid.to_excel(excel_out, index=False)

    # è¾“å‡ºæœªåŒ¹é… Shapefile
    if not unmatched_gdf.empty:
        shp_out = os.path.join(output_dir, f'batch_{i + 1}_remain_unmatched.shp')
        unmatched_gdf.to_file(shp_out)

    print(f"âœ… æ‰¹æ¬¡ {i + 1} å¤„ç†å®Œæˆï¼ŒåŒ¹é…æ•°: {len(matched_valid)}ï¼ŒæœªåŒ¹é…æ•°: {len(unmatched_gdf)}")

print("ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼")

"""
ç¬¬å››æ­¥ï¼šè·ç¦»åˆ¤æ–­æ³•ï¼ˆå« remain.shp è¾“å‡ºï¼‰
"""
import geopandas as gpd
import pandas as pd
from tqdm import tqdm
from shapely.ops import nearest_points

# ========= è·¯å¾„ =========
lake_path   = r'G:\ren\æ¹–æ³Šç”Ÿæ€åŒºç»Ÿè®¡\hydrolake_ecoregionbatches\lake_remain\lakeremain3\remain3.shp'
eco_path    = r'G:\ren\CBR\Ecoregions\ecoregions_dissolved_by_biome.shp'
out_csv     = r'G:\ren\æ¹–æ³Šç”Ÿæ€åŒºç»Ÿè®¡\hydrolake_ecoregionbatches\lake_remain\lakeremain3\lake_remain_assigned_by_distance.csv'
remain_path = r'G:\ren\æ¹–æ³Šç”Ÿæ€åŒºç»Ÿè®¡\hydrolake_ecoregionbatches\lake_remain\lakeremain3\remain4.shp'

# ========= è¯»æ•° & æŠ•å½± =========
lakes       = gpd.read_file(lake_path).to_crs(epsg=4087)          # World Equidistant Cylindrical
ecoregions  = gpd.read_file(eco_path).to_crs(epsg=4087)

# ========= ç»“æœå®¹å™¨ =========
assigned_rows   = []          # æœ€ç»ˆå±æ€§è¡¨
remain_lakes    = []          # ä»…â€œè·ç¦»åŒ¹é…â€æ¹–æ³Š

# ========= ä¸»å¾ªç¯ =========
for idx, lake in tqdm(lakes.iterrows(), total=len(lakes), desc='Assigning ecoregions'):
    lake_geom = lake.geometry
    
    # â‘  å°è¯•â€œç›¸äº¤â€åŒ¹é…
    intersected = ecoregions[ecoregions.intersects(lake_geom)]
    
    if not intersected.empty:
        chosen_eco = intersected.iloc[0]          # å¤šä¸ªæ—¶å¯æ”¹ä¸ºé¢ç§¯æœ€å¤§ç­‰ç­–ç•¥
    else:
        # â‘¡ è·ç¦»åŒ¹é…ï¼Œå¹¶è®°å½•åˆ° remain_lakes
        ecoregions['distance'] = ecoregions.geometry.boundary.distance(lake_geom)
        chosen_eco  = ecoregions.loc[ecoregions['distance'].idxmin()]
        remain_lakes.append(lake)                 # ä»…â€œè·ç¦»åŒ¹é…â€çš„æ¹–æ³Š
    
    # ==== åˆå¹¶å±æ€§ï¼ˆä¿ç•™æ¹–æ³Šå­—æ®µ + ç”Ÿæ€åŒºå­—æ®µï¼‰====
    merged_record = lake.drop(labels='geometry').copy()
    for col in ecoregions.columns:
        if col != 'geometry':
            merged_record[f'eco_{col}'] = chosen_eco[col]
    assigned_rows.append(merged_record)

# ========= è¾“å‡ºå±æ€§ CSV =========
pd.DataFrame(assigned_rows).to_csv(out_csv, index=False, encoding='utf-8-sig')
print(f"âœ… å±æ€§ç»“æœå·²ä¿å­˜ï¼š{out_csv}")

# ========= è¾“å‡º remain.shp =========
if remain_lakes:
    gpd.GeoDataFrame(remain_lakes, crs=lakes.crs).to_file(remain_path)
    print(f"âœ… é€šè¿‡â€œè·ç¦»åŒ¹é…â€çš„æ¹–æ³Šå·²è¾“å‡ºï¼š{remain_path}")
else:
    print("ğŸ‰ æ²¡æœ‰æ¹–æ³Šé€šè¿‡è·ç¦»æ‰è¢«åŒ¹é…ï¼Œremain.shp æœªç”Ÿæˆã€‚")



